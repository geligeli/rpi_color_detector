{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b6f670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 06:05:38.983311: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-04-20 06:05:38.983328: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-04-20 06:05:45.305523: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2022-04-20 06:05:45.307855: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1764] CUPTI activity buffer flushed\n",
      "2022-04-20 06:05:45.401928: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:521]  GpuTracer has collected 1020 callback api events and 1020 activity events. \n",
      "2022-04-20 06:05:45.457974: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-04-20 06:05:45.703399: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /tmp/logs/plugins/profile/2022_04_20_06_05_45\n",
      "\n",
      "2022-04-20 06:05:45.893199: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /tmp/logs/plugins/profile/2022_04_20_06_05_45/geli-9900k-1080ti.trace.json.gz\n",
      "2022-04-20 06:05:45.994257: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /tmp/logs/plugins/profile/2022_04_20_06_05_45\n",
      "\n",
      "2022-04-20 06:05:45.998728: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /tmp/logs/plugins/profile/2022_04_20_06_05_45/geli-9900k-1080ti.memory_profile.json.gz\n",
      "2022-04-20 06:05:46.000340: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /tmp/logs/plugins/profile/2022_04_20_06_05_45\n",
      "Dumped tool data for xplane.pb to /tmp/logs/plugins/profile/2022_04_20_06_05_45/geli-9900k-1080ti.xplane.pb\n",
      "Dumped tool data for overview_page.pb to /tmp/logs/plugins/profile/2022_04_20_06_05_45/geli-9900k-1080ti.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to /tmp/logs/plugins/profile/2022_04_20_06_05_45/geli-9900k-1080ti.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to /tmp/logs/plugins/profile/2022_04_20_06_05_45/geli-9900k-1080ti.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to /tmp/logs/plugins/profile/2022_04_20_06_05_45/geli-9900k-1080ti.kernel_stats.pb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "all_dataset = tf.data.Dataset.list_files(\n",
    "    \"/nfs/general/shared/Key*/*.jpg\").shuffle(buffer_size=10000)\n",
    "\n",
    "keys_tensor = tf.constant(['KeyA', 'KeyD'])\n",
    "vals_tensor = tf.constant([0, 1])\n",
    "init = tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor)\n",
    "table = tf.lookup.StaticHashTable(init, default_value=-1)\n",
    "\n",
    "def CreateImageAndLabel(filename):\n",
    "    image = tf.io.read_file(filename)\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "    image.set_shape([480, 640, 3])\n",
    "    # image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    labelString = tf.strings.split(filename, '/')[4]\n",
    "    # label = tf.one_hot(table.lookup(labelString),2)\n",
    "    return image, table.lookup(labelString)\n",
    "\n",
    "def ExtractRoiForPrediction(image):\n",
    "    return tf.image.crop_to_bounding_box(image, 220, 260, 40, 80)\n",
    "\n",
    "\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomTranslation(0.1, 0.1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "test_dataset = all_dataset.take(1000)\n",
    "test_dataset = test_dataset.map(CreateImageAndLabel, num_parallel_calls=AUTOTUNE,deterministic=False)\n",
    "test_dataset = test_dataset.batch(16)\n",
    "test_dataset = test_dataset.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=AUTOTUNE, deterministic=False)\n",
    "test_dataset = test_dataset.map(lambda x, y: (ExtractRoiForPrediction(x), y), num_parallel_calls=AUTOTUNE, deterministic=False)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "train_dataset = all_dataset.skip(1000)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.map(CreateImageAndLabel, num_parallel_calls=AUTOTUNE,deterministic=False)\n",
    "train_dataset = train_dataset.batch(16)\n",
    "train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=AUTOTUNE, deterministic=False)\n",
    "train_dataset = train_dataset.map(lambda x, y: (ExtractRoiForPrediction(x), y), num_parallel_calls=AUTOTUNE, deterministic=False)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "# test_dataset = all_dataset.take(1000).map(CreateImageAndLabel, num_parallel_calls=tf.data.AUTOTUNE,\n",
    "#                                           deterministic=False).batch(\n",
    "#     16).map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=tf.data.AUTOTUNE,\n",
    "#             deterministic=False).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# train_dataset = all_dataset.skip(1000).repeat().map(CreateImageAndLabel, num_parallel_calls=tf.data.AUTOTUNE,\n",
    "#                                                     deterministic=False).batch(\n",
    "#     16).map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=tf.data.AUTOTUNE, deterministic=False).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(8, (5, 5), padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),  # 'adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "    '/tmp/logs', update_freq=1, histogram_freq=1, write_images=True)\n",
    "\n",
    "checkpoint_filepath = '/tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=False)\n",
    "\n",
    "# help(model.fit)\n",
    "\n",
    "tf.profiler.experimental.start('/tmp/logs')\n",
    "# for i,j in enumerate(train_dataset):\n",
    "#     if i > 500:\n",
    "#         break\n",
    "model.fit(train_dataset, epochs=5, steps_per_epoch=256,\n",
    "          validation_data=test_dataset, callbacks=[tb_callback, model_checkpoint_callback])\n",
    "tf.profiler.experimental.stop()\n",
    "\n",
    "# print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd6b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(model, '/nfs/general/shared/tf_models/color_classifier')\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('/nfs/general/shared/tf_models/color_classifier') # path to the SavedModel directory\n",
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "#   tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "tflite_model = converter.convert()\n",
    "open('/nfs/general/shared/tflite/color_classifier.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "002994b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api._v2.keras.layers' has no attribute 'preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mpreprocessing\u001b[39m.\u001b[39mimage_preprocessing\u001b[39m.\u001b[39mtransform\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.api._v2.keras.layers' has no attribute 'preprocessing'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.layers.preprocessing.image_preprocessing.transform\n",
    "#help(tf.keras.layers.RandomZoom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23d6468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend\n",
    "from keras.engine import base_layer\n",
    "from keras.engine import base_preprocessing_layer\n",
    "from keras.layers.preprocessing import preprocessing_utils as utils\n",
    "from keras.utils import control_flow_util\n",
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "\n",
    "H_AXIS = -3\n",
    "W_AXIS = -2\n",
    "\n",
    "\n",
    "def check_fill_mode_and_interpolation(fill_mode, interpolation):\n",
    "  if fill_mode not in {'reflect', 'wrap', 'constant', 'nearest'}:\n",
    "    raise NotImplementedError(\n",
    "        'Unknown `fill_mode` {}. Only `reflect`, `wrap`, '\n",
    "        '`constant` and `nearest` are supported.'.format(fill_mode))\n",
    "  if interpolation not in {'nearest', 'bilinear'}:\n",
    "    raise NotImplementedError('Unknown `interpolation` {}. Only `nearest` and '\n",
    "                              '`bilinear` are supported.'.format(interpolation))\n",
    "\n",
    "\n",
    "class RandomTranslationAndRotation(base_layer.BaseRandomLayer):\n",
    "  \"\"\"A preprocessing layer which randomly translates images during training.\n",
    "\n",
    "  This layer will apply random translations to each image during training,\n",
    "  filling empty space according to `fill_mode`.\n",
    "\n",
    "  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n",
    "  of interger or floating point dtype. By default, the layer will output floats.\n",
    "\n",
    "  For an overview and full list of preprocessing layers, see the preprocessing\n",
    "  [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n",
    "\n",
    "  Args:\n",
    "    height_factor: a float represented as fraction of value, or a tuple of size\n",
    "      2 representing lower and upper bound for shifting vertically. A negative\n",
    "      value means shifting image up, while a positive value means shifting image\n",
    "      down. When represented as a single positive float, this value is used for\n",
    "      both the upper and lower bound. For instance, `height_factor=(-0.2, 0.3)`\n",
    "      results in an output shifted by a random amount in the range\n",
    "      `[-20%, +30%]`.\n",
    "      `height_factor=0.2` results in an output height shifted by a random amount\n",
    "      in the range `[-20%, +20%]`.\n",
    "    width_factor: a float represented as fraction of value, or a tuple of size 2\n",
    "      representing lower and upper bound for shifting horizontally. A negative\n",
    "      value means shifting image left, while a positive value means shifting\n",
    "      image right. When represented as a single positive float, this value is\n",
    "      used for both the upper and lower bound. For instance,\n",
    "      `width_factor=(-0.2, 0.3)` results in an output shifted left by 20%, and\n",
    "      shifted right by 30%. `width_factor=0.2` results in an output height\n",
    "      shifted left or right by 20%.\n",
    "    fill_mode: Points outside the boundaries of the input are filled according\n",
    "      to the given mode (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n",
    "      - *reflect*: `(d c b a | a b c d | d c b a)` The input is extended by\n",
    "        reflecting about the edge of the last pixel.\n",
    "      - *constant*: `(k k k k | a b c d | k k k k)` The input is extended by\n",
    "        filling all values beyond the edge with the same constant value k = 0.\n",
    "      - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n",
    "        wrapping around to the opposite edge.\n",
    "      - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by the\n",
    "        nearest pixel.\n",
    "    interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n",
    "      `\"bilinear\"`.\n",
    "    seed: Integer. Used to create a random seed.\n",
    "    fill_value: a float represents the value to be filled outside the boundaries\n",
    "      when `fill_mode=\"constant\"`.\n",
    "\n",
    "  Input shape:\n",
    "    3D (unbatched) or 4D (batched) tensor with shape:\n",
    "    `(..., height, width, channels)`,  in `\"channels_last\"` format.\n",
    "\n",
    "  Output shape:\n",
    "    3D (unbatched) or 4D (batched) tensor with shape:\n",
    "    `(..., height, width, channels)`,  in `\"channels_last\"` format.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               height_factor,\n",
    "               width_factor,\n",
    "               fill_mode='reflect',\n",
    "               interpolation='bilinear',\n",
    "               seed=None,\n",
    "               fill_value=0.0,\n",
    "               **kwargs):\n",
    "    base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomTranslation').set(\n",
    "        True)\n",
    "    super(RandomTranslation, self).__init__(seed=seed, force_generator=True,\n",
    "                                            **kwargs)\n",
    "    self.height_factor = height_factor\n",
    "    if isinstance(height_factor, (tuple, list)):\n",
    "      self.height_lower = height_factor[0]\n",
    "      self.height_upper = height_factor[1]\n",
    "    else:\n",
    "      self.height_lower = -height_factor\n",
    "      self.height_upper = height_factor\n",
    "    if self.height_upper < self.height_lower:\n",
    "      raise ValueError('`height_factor` cannot have upper bound less than '\n",
    "                       'lower bound, got {}'.format(height_factor))\n",
    "    if abs(self.height_lower) > 1. or abs(self.height_upper) > 1.:\n",
    "      raise ValueError('`height_factor` must have values between [-1, 1], '\n",
    "                       'got {}'.format(height_factor))\n",
    "\n",
    "    self.width_factor = width_factor\n",
    "    if isinstance(width_factor, (tuple, list)):\n",
    "      self.width_lower = width_factor[0]\n",
    "      self.width_upper = width_factor[1]\n",
    "    else:\n",
    "      self.width_lower = -width_factor\n",
    "      self.width_upper = width_factor\n",
    "    if self.width_upper < self.width_lower:\n",
    "      raise ValueError('`width_factor` cannot have upper bound less than '\n",
    "                       'lower bound, got {}'.format(width_factor))\n",
    "    if abs(self.width_lower) > 1. or abs(self.width_upper) > 1.:\n",
    "      raise ValueError('`width_factor` must have values between [-1, 1], '\n",
    "                       'got {}'.format(width_factor))\n",
    "\n",
    "    check_fill_mode_and_interpolation(fill_mode, interpolation)\n",
    "\n",
    "    self.fill_mode = fill_mode\n",
    "    self.fill_value = fill_value\n",
    "    self.interpolation = interpolation\n",
    "    self.seed = seed\n",
    "\n",
    "  def call(self, inputs, training=True):\n",
    "    if training is None:\n",
    "      training = backend.learning_phase()\n",
    "\n",
    "    inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n",
    "    original_shape = inputs.shape\n",
    "    unbatched = inputs.shape.rank == 3\n",
    "    # The transform op only accepts rank 4 inputs, so if we have an unbatched\n",
    "    # image, we need to temporarily expand dims to a batch.\n",
    "    if unbatched:\n",
    "      inputs = tf.expand_dims(inputs, 0)\n",
    "\n",
    "    def random_translated_inputs():\n",
    "      \"\"\"Translated inputs with random ops.\"\"\"\n",
    "      inputs_shape = tf.shape(inputs)\n",
    "      batch_size = inputs_shape[0]\n",
    "      img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)\n",
    "      img_wd = tf.cast(inputs_shape[W_AXIS], tf.float32)\n",
    "      height_translate = self._random_generator.random_uniform(\n",
    "          shape=[batch_size, 1],\n",
    "          minval=self.height_lower,\n",
    "          maxval=self.height_upper,\n",
    "          dtype=tf.float32)\n",
    "      height_translate = height_translate * img_hd\n",
    "      width_translate = self._random_generator.random_uniform(\n",
    "          shape=[batch_size, 1],\n",
    "          minval=self.width_lower,\n",
    "          maxval=self.width_upper,\n",
    "          dtype=tf.float32)\n",
    "      width_translate = width_translate * img_wd\n",
    "      translations = tf.cast(\n",
    "          tf.concat([width_translate, height_translate], axis=1),\n",
    "          dtype=tf.float32)\n",
    "      return transform(\n",
    "          inputs,\n",
    "          get_translation_matrix(translations),\n",
    "          interpolation=self.interpolation,\n",
    "          fill_mode=self.fill_mode,\n",
    "          fill_value=self.fill_value)\n",
    "\n",
    "    output = control_flow_util.smart_cond(training, random_translated_inputs,\n",
    "                                          lambda: inputs)\n",
    "    if unbatched:\n",
    "      output = tf.squeeze(output, 0)\n",
    "    output.set_shape(original_shape)\n",
    "    return output\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    return input_shape\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        'height_factor': self.height_factor,\n",
    "        'width_factor': self.width_factor,\n",
    "        'fill_mode': self.fill_mode,\n",
    "        'fill_value': self.fill_value,\n",
    "        'interpolation': self.interpolation,\n",
    "        'seed': self.seed,\n",
    "    }\n",
    "    base_config = super(RandomTranslation, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def get_translation_and_rotation_matrix(translations, rotation, name=None):\n",
    "  \"\"\"Returns projective transform(s) for the given translation(s).\n",
    "\n",
    "  Args:\n",
    "    translations: A matrix of 2-element lists representing `[dx, dy]`\n",
    "      to translate for each image (for a batch of images).\n",
    "    name: The name of the op.\n",
    "\n",
    "  Returns:\n",
    "    A tensor of shape `(num_images, 8)` projective transforms which can be given\n",
    "      to `transform`.\n",
    "  \"\"\"\n",
    "  with backend.name_scope(name or 'translation_matrix'):\n",
    "    num_translations = tf.shape(translations)[0]\n",
    "    # The translation matrix looks like:\n",
    "    #     [[1 0 -dx]\n",
    "    #      [0 1 -dy]\n",
    "    #      [0 0 1]]\n",
    "    # where the last entry is implicit.\n",
    "    # Translation matrices are always float32.\n",
    "    return tf.concat(\n",
    "        values=[\n",
    "            tf.ones((num_translations, 1), tf.float32),\n",
    "            tf.zeros((num_translations, 1), tf.float32),\n",
    "            -translations[:, 0, None],\n",
    "            tf.zeros((num_translations, 1), tf.float32),\n",
    "            tf.ones((num_translations, 1), tf.float32),\n",
    "            -translations[:, 1, None],\n",
    "            tf.zeros((num_translations, 2), tf.float32),\n",
    "        ],\n",
    "        axis=1)\n",
    "\n",
    "\n",
    "def transform(images,\n",
    "              transforms,\n",
    "              fill_mode='reflect',\n",
    "              fill_value=0.0,\n",
    "              interpolation='bilinear',\n",
    "              output_shape=None,\n",
    "              name=None):\n",
    "  \"\"\"Applies the given transform(s) to the image(s).\n",
    "\n",
    "  Args:\n",
    "    images: A tensor of shape\n",
    "      `(num_images, num_rows, num_columns, num_channels)` (NHWC). The rank must\n",
    "      be statically known (the shape is not `TensorShape(None)`).\n",
    "    transforms: Projective transform matrix/matrices. A vector of length 8 or\n",
    "      tensor of size N x 8. If one row of transforms is [a0, a1, a2, b0, b1, b2,\n",
    "      c0, c1], then it maps the *output* point `(x, y)` to a transformed *input*\n",
    "      point `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`, where\n",
    "      `k = c0 x + c1 y + 1`. The transforms are *inverted* compared to the\n",
    "      transform mapping input points to output points. Note that gradients are\n",
    "      not backpropagated into transformation parameters.\n",
    "    fill_mode: Points outside the boundaries of the input are filled according\n",
    "      to the given mode (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n",
    "    fill_value: a float represents the value to be filled outside the boundaries\n",
    "      when `fill_mode=\"constant\"`.\n",
    "    interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n",
    "      `\"bilinear\"`.\n",
    "    output_shape: Output dimension after the transform, `[height, width]`.\n",
    "      If `None`, output is the same size as input image.\n",
    "    name: The name of the op.\n",
    "\n",
    "  Fill mode behavior for each valid value is as follows:\n",
    "\n",
    "  - reflect (d c b a | a b c d | d c b a)\n",
    "  The input is extended by reflecting about the edge of the last pixel.\n",
    "\n",
    "  - constant (k k k k | a b c d | k k k k)\n",
    "  The input is extended by filling all\n",
    "  values beyond the edge with the same constant value k = 0.\n",
    "\n",
    "  - wrap (a b c d | a b c d | a b c d)\n",
    "  The input is extended by wrapping around to the opposite edge.\n",
    "\n",
    "  - nearest (a a a a | a b c d | d d d d)\n",
    "  The input is extended by the nearest pixel.\n",
    "\n",
    "  Input shape:\n",
    "    4D tensor with shape: `(samples, height, width, channels)`,\n",
    "      in `\"channels_last\"` format.\n",
    "\n",
    "  Output shape:\n",
    "    4D tensor with shape: `(samples, height, width, channels)`,\n",
    "      in `\"channels_last\"` format.\n",
    "\n",
    "  Returns:\n",
    "    Image(s) with the same type and shape as `images`, with the given\n",
    "    transform(s) applied. Transformed coordinates outside of the input image\n",
    "    will be filled with zeros.\n",
    "\n",
    "  Raises:\n",
    "    TypeError: If `image` is an invalid type.\n",
    "    ValueError: If output shape is not 1-D int32 Tensor.\n",
    "  \"\"\"\n",
    "  with backend.name_scope(name or 'transform'):\n",
    "    if output_shape is None:\n",
    "      output_shape = tf.shape(images)[1:3]\n",
    "      if not tf.executing_eagerly():\n",
    "        output_shape_value = tf.get_static_value(output_shape)\n",
    "        if output_shape_value is not None:\n",
    "          output_shape = output_shape_value\n",
    "\n",
    "    output_shape = tf.convert_to_tensor(\n",
    "        output_shape, tf.int32, name='output_shape')\n",
    "\n",
    "    if not output_shape.get_shape().is_compatible_with([2]):\n",
    "      raise ValueError('output_shape must be a 1-D Tensor of 2 elements: '\n",
    "                       'new_height, new_width, instead got '\n",
    "                       '{}'.format(output_shape))\n",
    "\n",
    "    fill_value = tf.convert_to_tensor(\n",
    "        fill_value, tf.float32, name='fill_value')\n",
    "\n",
    "    return tf.raw_ops.ImageProjectiveTransformV3(\n",
    "        images=images,\n",
    "        output_shape=output_shape,\n",
    "        fill_value=fill_value,\n",
    "        transforms=transforms,\n",
    "        fill_mode=fill_mode.upper(),\n",
    "        interpolation=interpolation.upper())\n",
    "\n",
    "\n",
    "def get_rotation_matrix(angles, image_height, image_width, name=None):\n",
    "  \"\"\"Returns projective transform(s) for the given angle(s).\n",
    "\n",
    "  Args:\n",
    "    angles: A scalar angle to rotate all images by, or (for batches of images) a\n",
    "      vector with an angle to rotate each image in the batch. The rank must be\n",
    "      statically known (the shape is not `TensorShape(None)`).\n",
    "    image_height: Height of the image(s) to be transformed.\n",
    "    image_width: Width of the image(s) to be transformed.\n",
    "    name: The name of the op.\n",
    "\n",
    "  Returns:\n",
    "    A tensor of shape (num_images, 8). Projective transforms which can be given\n",
    "      to operation `image_projective_transform_v2`. If one row of transforms is\n",
    "       [a0, a1, a2, b0, b1, b2, c0, c1], then it maps the *output* point\n",
    "       `(x, y)` to a transformed *input* point\n",
    "       `(x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k)`,\n",
    "       where `k = c0 x + c1 y + 1`.\n",
    "  \"\"\"\n",
    "  with backend.name_scope(name or 'rotation_matrix'):\n",
    "    x_offset = ((image_width - 1) - (tf.cos(angles) *\n",
    "                                     (image_width - 1) - tf.sin(angles) *\n",
    "                                     (image_height - 1))) / 2.0\n",
    "    y_offset = ((image_height - 1) - (tf.sin(angles) *\n",
    "                                      (image_width - 1) + tf.cos(angles) *\n",
    "                                      (image_height - 1))) / 2.0\n",
    "    num_angles = tf.shape(angles)[0]\n",
    "    return tf.concat(\n",
    "        values=[\n",
    "            tf.cos(angles)[:, None],\n",
    "            -tf.sin(angles)[:, None],\n",
    "            x_offset[:, None],\n",
    "            tf.sin(angles)[:, None],\n",
    "            tf.cos(angles)[:, None],\n",
    "            y_offset[:, None],\n",
    "            tf.zeros((num_angles, 2), tf.float32),\n",
    "        ],\n",
    "        axis=1)\n",
    "\n",
    "\n",
    "@keras_export('keras.layers.RandomRotation',\n",
    "              'keras.layers.experimental.preprocessing.RandomRotation')\n",
    "class RandomRotation(base_layer.BaseRandomLayer):\n",
    "  \"\"\"A preprocessing layer which randomly rotates images during training.\n",
    "\n",
    "  This layer will apply random rotations to each image, filling empty space\n",
    "  according to `fill_mode`.\n",
    "\n",
    "  By default, random rotations are only applied during training.\n",
    "  At inference time, the layer does nothing. If you need to apply random\n",
    "  rotations at inference time, set `training` to True when calling the layer.\n",
    "\n",
    "  Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n",
    "  of interger or floating point dtype. By default, the layer will output floats.\n",
    "\n",
    "  For an overview and full list of preprocessing layers, see the preprocessing\n",
    "  [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n",
    "\n",
    "  Input shape:\n",
    "    3D (unbatched) or 4D (batched) tensor with shape:\n",
    "    `(..., height, width, channels)`, in `\"channels_last\"` format\n",
    "\n",
    "  Output shape:\n",
    "    3D (unbatched) or 4D (batched) tensor with shape:\n",
    "    `(..., height, width, channels)`, in `\"channels_last\"` format\n",
    "\n",
    "  Attributes:\n",
    "    factor: a float represented as fraction of 2 Pi, or a tuple of size 2\n",
    "      representing lower and upper bound for rotating clockwise and\n",
    "      counter-clockwise. A positive values means rotating counter clock-wise,\n",
    "      while a negative value means clock-wise. When represented as a single\n",
    "      float, this value is used for both the upper and lower bound. For\n",
    "      instance, `factor=(-0.2, 0.3)` results in an output rotation by a random\n",
    "      amount in the range `[-20% * 2pi, 30% * 2pi]`. `factor=0.2` results in an\n",
    "      output rotating by a random amount in the range `[-20% * 2pi, 20% * 2pi]`.\n",
    "    fill_mode: Points outside the boundaries of the input are filled according\n",
    "      to the given mode (one of `{\"constant\", \"reflect\", \"wrap\", \"nearest\"}`).\n",
    "      - *reflect*: `(d c b a | a b c d | d c b a)` The input is extended by\n",
    "        reflecting about the edge of the last pixel.\n",
    "      - *constant*: `(k k k k | a b c d | k k k k)` The input is extended by\n",
    "        filling all values beyond the edge with the same constant value k = 0.\n",
    "      - *wrap*: `(a b c d | a b c d | a b c d)` The input is extended by\n",
    "        wrapping around to the opposite edge.\n",
    "      - *nearest*: `(a a a a | a b c d | d d d d)` The input is extended by the\n",
    "        nearest pixel.\n",
    "    interpolation: Interpolation mode. Supported values: `\"nearest\"`,\n",
    "      `\"bilinear\"`.\n",
    "    seed: Integer. Used to create a random seed.\n",
    "    fill_value: a float represents the value to be filled outside the boundaries\n",
    "      when `fill_mode=\"constant\"`.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               factor,\n",
    "               fill_mode='reflect',\n",
    "               interpolation='bilinear',\n",
    "               seed=None,\n",
    "               fill_value=0.0,\n",
    "               **kwargs):\n",
    "    base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomRotation').set(\n",
    "        True)\n",
    "    super(RandomRotation, self).__init__(seed=seed, force_generator=True,\n",
    "                                         **kwargs)\n",
    "    self.factor = factor\n",
    "    if isinstance(factor, (tuple, list)):\n",
    "      self.lower = factor[0]\n",
    "      self.upper = factor[1]\n",
    "    else:\n",
    "      self.lower = -factor\n",
    "      self.upper = factor\n",
    "    if self.upper < self.lower:\n",
    "      raise ValueError('Factor cannot have negative values, '\n",
    "                       'got {}'.format(factor))\n",
    "    check_fill_mode_and_interpolation(fill_mode, interpolation)\n",
    "    self.fill_mode = fill_mode\n",
    "    self.fill_value = fill_value\n",
    "    self.interpolation = interpolation\n",
    "    self.seed = seed\n",
    "\n",
    "  def call(self, inputs, training=True):\n",
    "    if training is None:\n",
    "      training = backend.learning_phase()\n",
    "\n",
    "    inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n",
    "    original_shape = inputs.shape\n",
    "    unbatched = inputs.shape.rank == 3\n",
    "    # The transform op only accepts rank 4 inputs, so if we have an unbatched\n",
    "    # image, we need to temporarily expand dims to a batch.\n",
    "    if unbatched:\n",
    "      inputs = tf.expand_dims(inputs, 0)\n",
    "\n",
    "    def random_rotated_inputs():\n",
    "      \"\"\"Rotated inputs with random ops.\"\"\"\n",
    "      inputs_shape = tf.shape(inputs)\n",
    "      batch_size = inputs_shape[0]\n",
    "      img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)\n",
    "      img_wd = tf.cast(inputs_shape[W_AXIS], tf.float32)\n",
    "      min_angle = self.lower * 2. * np.pi\n",
    "      max_angle = self.upper * 2. * np.pi\n",
    "      angles = self._random_generator.random_uniform(\n",
    "          shape=[batch_size], minval=min_angle, maxval=max_angle)\n",
    "      return transform(\n",
    "          inputs,\n",
    "          get_rotation_matrix(angles, img_hd, img_wd),\n",
    "          fill_mode=self.fill_mode,\n",
    "          fill_value=self.fill_value,\n",
    "          interpolation=self.interpolation)\n",
    "\n",
    "    output = control_flow_util.smart_cond(training, random_rotated_inputs,\n",
    "                                          lambda: inputs)\n",
    "    if unbatched:\n",
    "      output = tf.squeeze(output, 0)\n",
    "    output.set_shape(original_shape)\n",
    "    return output\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    return input_shape\n",
    "\n",
    "  def get_config(self):\n",
    "    config = {\n",
    "        'factor': self.factor,\n",
    "        'fill_mode': self.fill_mode,\n",
    "        'fill_value': self.fill_value,\n",
    "        'interpolation': self.interpolation,\n",
    "        'seed': self.seed,\n",
    "    }\n",
    "    base_config = super(RandomRotation, self).get_config()\n",
    "    return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb152ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
